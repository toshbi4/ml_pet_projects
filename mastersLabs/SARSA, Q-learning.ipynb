{"cells":[{"cell_type":"markdown","metadata":{"id":"q9QLe_T6GZUd"},"source":["# Programming task"]},{"cell_type":"markdown","metadata":{"id":"EYlIf2yHv8hz"},"source":["**The task should be completed with the current values of the hyperparameters. For verification, below you will see the answers that should be obtained. After all the answers match, you can use the resulting notebook to complete your individual task.**"]},{"cell_type":"markdown","metadata":{"id":"ZDQzNIZXAoFE"},"source":["Set model hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOMw2ZbOAmOZ"},"outputs":[],"source":["epsilon = 0.1 # Epsilon parameter which is used in epsilon-greedy strategy\n","gamma = 0.8 # Discount coefficient gamma\n","random_seed = 9 #Random seed\n","time_delay = 1 # Time delay when rendering the game process after training (seconds)\n","lr_rate = 0.9 #Learning rate alpha"]},{"cell_type":"markdown","metadata":{"id":"pQu5IYHX8jId"},"source":["We import the libraries, create our own 6x6 environment. S denotes the starting point. F - ice is safe (frozen), H - hole, G - goal. The parameter `is_slippery = False` is responsible for slipping. That is, if the agent chose the action to go right, then it will move to the corresponding state. In the general case, due to the “slipping”, one may find itself in a different state. We also copied from the GYM library and slightly modified the function `` generate_random_map ``, in order to generate arbitrary maps based on `` random_seed ``."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":832,"status":"ok","timestamp":1652550202968,"user":{"displayName":"Антон Потапов","userId":"06523293842168985278"},"user_tz":-180},"id":"awL7CCCwD6C3","outputId":"33750c1b-6687-45b8-884c-d71d7b16ac0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your map\n","\n","\u001b[41mS\u001b[0mFFFFF\n","FFFFFH\n","HFFFHF\n","FFFFHF\n","HHHFFH\n","FHHFFG\n"]}],"source":["import gym\n","import numpy as np\n","import time\n","from IPython.display import clear_output\n","\n","\n","def generate_random_map(size, p, sd):\n","    \"\"\"Generates a random valid map (one that has a path from start to goal)\n","    :param size: size of each side of the grid\n","    :param p: probability that a tile is frozen\n","    \"\"\"\n","    valid = False\n","    np.random.seed(sd)\n","\n","    # DFS to check that it's a valid path.\n","    def is_valid(res):\n","        frontier, discovered = [], set()\n","        frontier.append((0,0))\n","        while frontier:\n","            r, c = frontier.pop()\n","            if not (r,c) in discovered:\n","                discovered.add((r,c))\n","                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","                for x, y in directions:\n","                    r_new = r + x\n","                    c_new = c + y\n","                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n","                        continue\n","                    if res[r_new][c_new] == 'G':\n","                        return True\n","                    if (res[r_new][c_new] not in '#H'):\n","                        frontier.append((r_new, c_new))\n","        return False\n","\n","    while not valid:\n","        p = min(1, p)\n","        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n","        res[0][0] = 'S'\n","        res[-1][-1] = 'G'\n","        valid = is_valid(res)\n","    return [\"\".join(x) for x in res]\n","\n","# Map generation\n","random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Create our map\n","env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Initialize environment\n","print(\"Your map\")\n","env.render() #Render the map"]},{"cell_type":"markdown","metadata":{"id":"MDCexoEU9a_c"},"source":["Functions for selecting an action and updating an action value table. The line *** is used to check responses in openedx. Outside of the academic problem, it is better to use the original method of the `environment` class, that is:\n","\n","`action = env.action_space.sample ()`"]},{"cell_type":"markdown","metadata":{"id":"D5TbDqn6G_Pt"},"source":["# Task 1\n","Complete the function `` learn () ``, so that as a result of its call, the value of the current action is updated according to the Q-learning algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"id":"CdQBpxaTOK7u","executionInfo":{"status":"error","timestamp":1652550202976,"user_tz":-180,"elapsed":27,"user":{"displayName":"Антон Потапов","userId":"06523293842168985278"}},"outputId":"c9e2d4c1-66ee-4309-b897-1934b8dab5f8"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-1e54a1494c37>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    #Q[state, action] = #Your code here\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}],"source":["def choose_action(state):\n","    action=0\n","    if np.random.uniform(0, 1) < epsilon:\n","        action = np.random.randint(0,env.action_space.n) #***\n","    else:\n","        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n","    return action\n","\n","def learn(state, state2, reward, action, done):\n","    #Q[state, action] = #Your code here"]},{"cell_type":"markdown","metadata":{"id":"7COGeyA_Ist3"},"source":["# Task 2\n","Complete the following code so that as a result of training the model you could find out the number of wins and the number of the game (`game`), on which the agent won the fifth victory in a row for the first time."]},{"cell_type":"markdown","metadata":{"id":"0adDl7NvJoQP"},"source":["Let's explain what the function ```env.step(action)``` returns \n","\n","```state2``` --  next state\n","\n","```reward``` -- reward\n","\n","```done``` -- inidcator of the end of the game. True in case of victory or fall into the hole. False in other cases.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aq92-dWiOchF"},"outputs":[],"source":["from tqdm import tqdm\n","# Inititalization\n","np.random.seed(random_seed)\n","total_games = 10000\n","max_steps = 100\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","#Main cycle\n","for game in tqdm(range(total_games)):\n","    state = env.reset()\n","    t = 0\n","    while t < max_steps:\n","        \n","        t += 1\n","\n","        action = choose_action(state)\n","\n","        state2, reward, done, info = env.step(action)\n","\n","        if t == max_steps:\n","          done = True  \n","\n","        learn(state, state2, reward, action, done)\n","\n","        state = state2\n","\n","        if done:\n","          break\n"]},{"cell_type":"markdown","metadata":{"id":"JFuxsqdRLOS9"},"source":["Output answers with the given parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZbJtFnhLa7w"},"outputs":[],"source":["print(\"The number of victories in a series of 10,000 games: \", #Your code here)\n","print(\"Five wins in a row were first won in the game \", #Your code here)\n"]},{"cell_type":"markdown","metadata":{"id":"TSXdSiG2WI71"},"source":["The following results should be obtained.\n","\n","\n","*  The number of victories in a series of 10,000 games:  7914\n","*  Five wins in a row were first won in the game  885\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nazZaAbwQGBt"},"source":["Let's perform one game to track the actions of the agent. At the same time, we will consider the model fully trained, that is, actions are selected according to the greedy strategy, the values of the actions in the table are not updated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ysllZjEQXLa"},"outputs":[],"source":["import time\n","#Greedy action selection\n","def choose_action_one_game(state):\n","    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n","    return action\n","\n","states=[]#Array to save agent states during the game\n","t = 0\n","state = env.reset()\n","wn = 0\n","while(t<100):\n","  env.render()\n","  time.sleep(time_delay)\n","  clear_output(wait=True)\n","  action = choose_action_one_game(state)  \n","  state2, reward, done, info = env.step(action)  \n","  states.append(state)\n","  state = state2\n","  t += 1\n","  if done and reward == 1:\n","    wn=1\n","  if done:\n","    break\n","if wn == 1:\n","  print(\"!!!WIN!!!\")"]},{"cell_type":"markdown","metadata":{"id":"x696NulpReFI"},"source":["Route map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKMCMdpOTcXy"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def make_maze_pic(maze):\n","  maze_pic=[]\n","  for i in range(len(maze)):\n","    row = []\n","    for j in range(len(maze[i])):\n","      if maze[i][j] == 'S':\n","        row.append(0)\n","      if maze[i][j] == 'F':\n","        row.append(0)\n","      if maze[i][j] == 'H':\n","        row.append(1)\n","      if maze[i][j] == 'G':\n","        row.append(0)\n","    maze_pic.append(row)\n","  maze_pic = np.array(maze_pic)\n","  return maze_pic\n","  \n","\n","#Make maze fit to plot\n","maze_pic = make_maze_pic(random_map)\n","nrows, ncols = maze_pic.shape\n","\n","#Arrays of picture elements\n","rw = np.remainder(states,nrows)\n","cl = np.floor_divide(states,nrows)\n","if wn == 1:\n","  rw = np.append(rw, [nrows-1])\n","  cl = np.append(cl,[ncols-1])\n","\n","#Picture plotting\n","fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n","ax1.clear()\n","ax1.set_xticks(np.arange(0.5, nrows, step=1))\n","ax1.set_xticklabels([])\n","ax1.set_yticks(np.arange(0.5, ncols, step=1))\n","ax1.set_yticklabels([])\n","ax1.grid(True)\n","ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n","ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n","ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n","ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n","ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n","ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n","ax1.imshow(maze_pic, cmap=\"binary\")"]},{"cell_type":"markdown","metadata":{"id":"5m14YFyrI6M0"},"source":["# Task 3"]},{"cell_type":"markdown","metadata":{"id":"lb3BvDuBxTO0"},"source":["Duplicate the resulting notebook and use the SARSA algorithm instead of the Q-learning algorithm. Please note that the task requires changing the number of games. That is, `total_games = 40000`. Blocks should be run sequentially from the very beginning (due to `random_seed`). We draw your attention to the fact that when changing the algorithm from Q-learning to SARSA, both the learning process and the `learn ()` function should be modified. Note that, the `learn ()` function should have an additional argument (the next action). Below you can find a \"sceleton\" of code explaining how to modify the algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSVpTwTAJO7d"},"outputs":[],"source":["from tqdm import tqdm\n","np.random.seed(random_seed)\n","total_games = 40000\n","max_steps = 100\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","#Main cycle\n","for game in tqdm(range(total_games)):\n","    state = env.reset()\n","    t = 0\n","    action = #Choice of action at the very beginning of each game\n","    while t < max_steps:\n","              \n","        t += 1\n","\n","        state2, reward, done, info = env.step(action)\n","\n","        action2 =  #choice of action for the next step of the game, as well as for updating the value of the current action\n","\n","        if t == max_steps:\n","          done = True  \n","\n","        learn(state, state2, reward, action, action2, done) # action2 is also passed to the training function\n","\n","        state = state2\n","\n","        action = action2\n","\n","        if done:\n","          break"]},{"cell_type":"markdown","metadata":{"id":"RB_PX2vYIY0-"},"source":["As a result of training, the following answers should be obtained:\n","\n","*   The number of victories in a series of 40,000 games:  32328\n","*   Five wins in a row were first won in the game  894"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}